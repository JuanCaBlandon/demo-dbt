# trigger:
#   branches:
#     include:
#       - main
#       - feature/ci-cd-pipeline

# variables:
#   - name: RUNTIME_ENV
#     value: $(System.StageName)
#   - group: databricks-credentials-latest

# pool:
#   vmImage: 'ubuntu-latest'

# stages:
# - stage: Deploy
#   jobs:
#   - job: DeployToDatabricks
#     steps:
#     - task: Bash@3
#       inputs:
#         targetType: 'inline'
#         script: |
#             # Create Databricks config file
#             mkdir -p ~/.databricks
#             cat > ~/.databrickscfg << EOF
#             [DEFAULT]
#             host = $(DATABRICKS_HOST)
#             azure_client_id = $(DATABRICKS_CLIENT_ID)
#             azure_client_secret = $(DATABRICKS_CLIENT_SECRET)
#             azure_tenant_id = 9c643eac-4572-41a3-8844-e4a079aa6409
#             EOF
            
#             # Debug: Check config file (with secrets redacted)
#             echo "Checking databricks config:"
#             sed -e 's/\(client_secret = \).*/\1[REDACTED]/' ~/.databrickscfg
            
#             # Install Databricks CLI
#             pip install databricks-cli --upgrade
            
#             # Create a temporary directory for the folders we want to copy
#             mkdir -p temp_deploy
#             cp -r $(Build.SourcesDirectory)/DatabricksArtifacts temp_deploy/
            
#             # Test Databricks connection
#             echo "Testing Databricks connection:"
#             databricks clusters list -p DEFAULT
            
#             # Remove existing content if any
#             echo "Removing existing content:"
#             databricks fs rm -r dbfs:/repositories/$(Build.Repository.Name) || true
            
#             # Copy the folders
#             echo "Copying files to DBFS:"
#             databricks fs cp -r temp_deploy/* dbfs:/repositories/$(Build.Repository.Name)
            
#             # Verify the copy
#             echo "Verifying copied files:"
#             databricks fs ls dbfs:/repositories/$(Build.Repository.Name)
            
#             # Clean up
#             rm -rf temp_deploy
#       env:
#         DATABRICKS_HOST: $(DATABRICKS_HOST)
#         DATABRICKS_ACCOUNT_ID: $(DATABRICKS_ACCOUNT_ID)
#         DATABRICKS_CLIENT_ID: $(DATABRICKS_CLIENT_ID)
#         DATABRICKS_CLIENT_SECRET: $(DATABRICKS_CLIENT_SECRET)
trigger:
  branches:
    include:
      - main
      - feature/ci-cd-pipeline

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: databricks-credentials-latest

steps:
- script: |
    python -m pip install --upgrade pip
    pip install --upgrade databricks-cli requests
    
    # Show installed versions
    pip list | grep -E "databricks-cli|requests|urllib3"
  displayName: 'Install Dependencies'

- script: |
    # Debug: Show environment
    echo "Home directory: $HOME"
    echo "Current user: $(whoami)"
    echo "Current directory: $(pwd)"
    
    # Ensure .databrickscfg directory exists
    echo "Creating config directory..."
    mkdir -p $HOME/.databrickscfg
    
    # Create config file
    CONFIG_FILE="$HOME/.databrickscfg/config"
    echo "Creating config file at: $CONFIG_FILE"
    
    # Create config with Azure AD SP authentication
    cat > "$CONFIG_FILE" << EOF
    [DEFAULT]
    host = $(DATABRICKS_HOST)
    azure_workspace_resource_id = $(DATABRICKS_WORKSPACE_RESOURCE_ID)
    azure_client_id = $(DATABRICKS_CLIENT_ID)
    azure_client_secret = $(DATABRICKS_CLIENT_SECRET)
    azure_tenant_id = $(DATABRICKS_TENANT_ID)
    EOF
    
    # Set permissions
    chmod 600 "$CONFIG_FILE"
    
    # Debug: Show config file location and contents
    echo "Config file exists: $(test -f "$CONFIG_FILE" && echo "Yes" || echo "No")"
    echo "Config file permissions: $(ls -l "$CONFIG_FILE")"
    echo "Config contents (redacted):"
    sed 's/azure_client_secret = .*/azure_client_secret = [REDACTED]/' "$CONFIG_FILE"
    
    # Export config file location
    export DATABRICKS_CONFIG_FILE="$CONFIG_FILE"
  displayName: 'Configure Databricks CLI'
  env:
    DATABRICKS_CONFIG_FILE: $(HOME)/.databrickscfg/config

- script: |
    # Export config file location again for this step
    export DATABRICKS_CONFIG_FILE="$HOME/.databrickscfg/config"
    
    # Debug: Show environment variables
    echo "DATABRICKS_CONFIG_FILE: $DATABRICKS_CONFIG_FILE"
    echo "Config file exists: $(test -f "$DATABRICKS_CONFIG_FILE" && echo "Yes" || echo "No")"
    
    # Test connection with verbose output
    echo "Testing Databricks connection..."
    databricks workspace list --debug
    
    # If successful, proceed with file operations
    echo "Creating DBFS directory..."
    databricks fs mkdirs "dbfs:/FileStore/my_project"
    
    # List directories
    echo "Source directory contents:"
    ls -la $(Build.SourcesDirectory)
    
    # Copy folders
    cd $(Build.SourcesDirectory)
    for folder in "workflows" "dbt_project"; do
      if [ -d "$folder" ]; then
        echo "Copying $folder to DBFS..."
        databricks fs cp -r "$folder" "dbfs:/FileStore/my_project/$folder"
      else
        echo "Warning: Source folder $folder not found"
      fi
    done
  displayName: 'Copy Files to DBFS'
  env:
    DATABRICKS_CONFIG_FILE: $(HOME)/.databrickscfg/config