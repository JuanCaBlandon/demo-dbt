trigger:
  branches:
    include:
      - main
      - feature/ci-cd-pipeline

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: databricks-credentials-latest

steps:
- script: |
    # Install required tools
    pip install --upgrade databricks-cli
  displayName: 'Install Dependencies'

- script: |
    # Generate OAuth token
    echo "Generating OAuth token..."
    TOKEN_ENDPOINT="https://accounts.cloud.databricks.com/oidc/accounts/$(DATABRICKS_ACCOUNT_ID)/v1/token"
    
    # Get OAuth token
    RESPONSE=$(curl --silent --request POST \
      --url "$TOKEN_ENDPOINT" \
      --user "$(DATABRICKS_CLIENT_ID):$(DATABRICKS_CLIENT_SECRET)" \
      --data 'grant_type=client_credentials&scope=all-apis')
    
    # Extract access token
    ACCESS_TOKEN=$(echo $RESPONSE | python -c "import sys, json; print(json.load(sys.stdin)['access_token'])")
    
    # Store token in environment variable for next steps
    echo "##vso[task.setvariable variable=DATABRICKS_TOKEN]$ACCESS_TOKEN"
    
    echo "OAuth token generated successfully"
  displayName: 'Generate OAuth Token'

- script: |
    # Configure Databricks CLI with the OAuth token
    echo "Configuring Databricks CLI..."
    mkdir -p ~/.databrickscfg
    
    # Create config file
    cat > ~/.databrickscfg << EOF
    [DEFAULT]
    host = $(DATABRICKS_HOST)
    token = $(DATABRICKS_TOKEN)
    EOF
    
    # Test connection
    echo "Testing Databricks connection..."
    databricks fs ls
  displayName: 'Configure and Test Databricks CLI'

- script: |
    # Create DBFS directory and copy files
    echo "Creating DBFS directory..."
    databricks fs mkdirs "dbfs:/repositories"
    
     # Show current directory and contents
    echo "Current directory: $(pwd)"
    echo "Directory contents:"
    ls -la
    
    # Check if dbt_project folder exists
    if [ -d "dbt_project" ]; then
        echo "Found dbt_project folder, copying to DBFS..."
        databricks fs cp -r dbt_project "dbfs:/FileStore/deployment/"
        
        # Verify the copy
        echo "Verifying copied files:"
        databricks fs ls "dbfs:/FileStore/deployment/dbt_project"
    else
        echo "Error: dbt_project folder not found in $(pwd)"
        echo "Full directory listing:"
        ls -R
    fi
  displayName: 'Copy Files to DBFS'