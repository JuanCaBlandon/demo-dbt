# trigger:
#   branches:
#     include:
#       - main
#       - feature/ci-cd-pipeline

# variables:
#   - name: RUNTIME_ENV
#     value: $(System.StageName)
#   - group: databricks-credentials-latest

# pool:
#   vmImage: 'ubuntu-latest'

# stages:
# - stage: Deploy
#   jobs:
#   - job: DeployToDatabricks
#     steps:
#     - task: Bash@3
#       inputs:
#         targetType: 'inline'
#         script: |
#             # Create Databricks config file
#             mkdir -p ~/.databricks
#             cat > ~/.databrickscfg << EOF
#             [DEFAULT]
#             host = $(DATABRICKS_HOST)
#             azure_client_id = $(DATABRICKS_CLIENT_ID)
#             azure_client_secret = $(DATABRICKS_CLIENT_SECRET)
#             azure_tenant_id = 9c643eac-4572-41a3-8844-e4a079aa6409
#             EOF
            
#             # Debug: Check config file (with secrets redacted)
#             echo "Checking databricks config:"
#             sed -e 's/\(client_secret = \).*/\1[REDACTED]/' ~/.databrickscfg
            
#             # Install Databricks CLI
#             pip install databricks-cli --upgrade
            
#             # Create a temporary directory for the folders we want to copy
#             mkdir -p temp_deploy
#             cp -r $(Build.SourcesDirectory)/DatabricksArtifacts temp_deploy/
            
#             # Test Databricks connection
#             echo "Testing Databricks connection:"
#             databricks clusters list -p DEFAULT
            
#             # Remove existing content if any
#             echo "Removing existing content:"
#             databricks fs rm -r dbfs:/repositories/$(Build.Repository.Name) || true
            
#             # Copy the folders
#             echo "Copying files to DBFS:"
#             databricks fs cp -r temp_deploy/* dbfs:/repositories/$(Build.Repository.Name)
            
#             # Verify the copy
#             echo "Verifying copied files:"
#             databricks fs ls dbfs:/repositories/$(Build.Repository.Name)
            
#             # Clean up
#             rm -rf temp_deploy
#       env:
#         DATABRICKS_HOST: $(DATABRICKS_HOST)
#         DATABRICKS_ACCOUNT_ID: $(DATABRICKS_ACCOUNT_ID)
#         DATABRICKS_CLIENT_ID: $(DATABRICKS_CLIENT_ID)
#         DATABRICKS_CLIENT_SECRET: $(DATABRICKS_CLIENT_SECRET)
trigger:
  branches:
    include:
      - main
      - feature/ci-cd-pipeline

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: databricks-credentials-latest  # Should contain CLIENT_ID and CLIENT_SECRET

steps:
- script: |
    pip install --upgrade databricks-cli
  displayName: 'Install Databricks CLI'

- script: |
    echo "Creating Databricks config file..."
    CONFIG_FILE="$HOME/.databrickscfg"
    
    # Remove if exists and create new config file
    rm -rf "$CONFIG_FILE"
    
    # Create the config file
    echo "[DEFAULT]" > "$CONFIG_FILE"
    echo "host = $(DATABRICKS_HOST)" >> "$CONFIG_FILE"
    echo "client_id = $(DATABRICKS_CLIENT_ID)" >> "$CONFIG_FILE"
    echo "client_secret = $(DATABRICKS_CLIENT_SECRET)" >> "$CONFIG_FILE"
    echo "tenant_id = 9c643eac-4572-41a3-8844-e4a079aa6409" >> "$CONFIG_FILE"
    
    # Set proper permissions
    chmod 600 "$CONFIG_FILE"

    # Debug: Show config file exists and its contents (without secrets)
    echo "Config file location: $CONFIG_FILE"
    echo "Config file exists: $(test -f "$CONFIG_FILE" && echo "Yes" || echo "No")"
    echo "Config file contents (redacted):"
    sed 's/client_secret = .*/client_secret = [REDACTED]/' "$CONFIG_FILE"
  displayName: 'Configure Databricks CLI'
  env:
    HOME: $(Pipeline.Workspace)

- script: |
    export DATABRICKS_CONFIG_FILE="$(Pipeline.Workspace)/.databrickscfg"
    
    # Test connection first
    echo "Testing Databricks connection..."
    databricks workspace list
    
    # Create base directory
    echo "Creating DBFS directory..."
    databricks fs mkdirs "dbfs:/FileStore/my_project"
    
    # Copy folders if they exist
    for folder in "workflows" "dbt_project"; do
      if [ -d "$folder" ]; then
        echo "Copying $folder to DBFS..."
        databricks fs cp -r "$folder" "dbfs:/FileStore/my_project/$folder"
      else
        echo "Warning: Source folder $folder not found in $(pwd)"
        echo "Directory contents:"
        ls -la
      fi
    done
    
    # List final contents
    echo "Final DBFS contents:"
    databricks fs ls "dbfs:/FileStore/my_project"
  displayName: 'Copy Files to DBFS'
  env:
    DATABRICKS_HOST: $(DATABRICKS_HOST)
    DATABRICKS_CLIENT_ID: $(DATABRICKS_CLIENT_ID)
    DATABRICKS_CLIENT_SECRET: $(DATABRICKS_CLIENT_SECRET)
    DATABRICKS_CONFIG_FILE: $(Pipeline.Workspace)/.databrickscfg