trigger:
  branches:
    include:
      - main
      - feature/ci-cd-pipeline

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: databricks-credentials-latest

steps:
- script: |
    # Install required tools
    pip install --upgrade databricks-cli
  displayName: 'Install Dependencies'

- script: |
    # Generate OAuth token
    echo "Generating OAuth token..."
    TOKEN_ENDPOINT="https://accounts.cloud.databricks.com/oidc/accounts/$(DATABRICKS_ACCOUNT_ID)/v1/token"
    
    # Get OAuth token
    RESPONSE=$(curl --silent --request POST \
      --url "$TOKEN_ENDPOINT" \
      --user "$(DATABRICKS_CLIENT_ID):$(DATABRICKS_CLIENT_SECRET)" \
      --data 'grant_type=client_credentials&scope=all-apis')
    
    # Extract access token
    ACCESS_TOKEN=$(echo $RESPONSE | python -c "import sys, json; print(json.load(sys.stdin)['access_token'])")
    
    # Store token in environment variable for next steps
    echo "##vso[task.setvariable variable=DATABRICKS_TOKEN]$ACCESS_TOKEN"
    
    echo "OAuth token generated successfully"
  displayName: 'Generate OAuth Token'

- script: |
    # Configure Databricks CLI with the OAuth token
    echo "Configuring Databricks CLI..."
    mkdir -p ~/.databrickscfg
    
    # Create config file
    cat > ~/.databrickscfg << EOF
    [DEFAULT]
    host = $(DATABRICKS_HOST)
    token = $(DATABRICKS_TOKEN)
    EOF
    
    # Test connection
    echo "Testing Databricks connection..."
    databricks fs ls
  displayName: 'Configure and Test Databricks CLI'

- script: |
    echo "Starting file copy process..."
    
    # Move to source directory
    cd $(Build.SourcesDirectory)
    
    # Show current directory and contents for debugging
    echo "Current directory: $(pwd)"
    echo "Directory contents:"
    ls -la
    
    # Create repositories directory in DBFS
    echo "Creating repositories directory in DBFS..."
    databricks fs mkdirs "dbfs:/repositories/$(Build.Repository.Name)"
    
    # Array of folders to copy
    FOLDERS=("dbt_project" "workflows")
    
    # Copy each folder
    for folder in "${FOLDERS[@]}"; do
        if [ -d "$folder" ]; then
            echo "Found $folder folder, copying to DBFS..."
            # Remove existing content if any
            databricks fs rm -r "dbfs:/repositories/$(Build.Repository.Name)/$folder" || true
            # Copy folder
            databricks fs cp -r "$folder" "dbfs:/repositories/$(Build.Repository.Name)/$folder"
            
            # Verify the copy
            echo "Verifying copied files for $folder:"
            databricks fs ls "dbfs:/repositories/$(Build.Repository.Name)/$folder"
        else
            echo "Warning: $folder folder not found in $(pwd)"
        fi
    done
    
    # Show final structure
    echo "Final repository structure in DBFS:"
    databricks fs ls "dbfs:/repositories/$(Build.Repository.Name)"
  displayName: 'Copy Files to DBFS'