trigger:
  branches:
    include:
      - main
      - feature/ci-cd-pipeline

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: databricks-credentials-latest

steps:
- script: |
    # Install required tools
    pip install --upgrade databricks-cli
  displayName: 'Install Dependencies'

- script: |
    # Generate OAuth token
    echo "Generating OAuth token..."
    TOKEN_ENDPOINT="https://accounts.cloud.databricks.com/oidc/accounts/$(DATABRICKS_ACCOUNT_ID)/v1/token"
    
    # Get OAuth token
    RESPONSE=$(curl --silent --request POST \
      --url "$TOKEN_ENDPOINT" \
      --user "$(DATABRICKS_CLIENT_ID):$(DATABRICKS_CLIENT_SECRET)" \
      --data 'grant_type=client_credentials&scope=all-apis')
    
    # Extract access token
    ACCESS_TOKEN=$(echo $RESPONSE | python -c "import sys, json; print(json.load(sys.stdin)['access_token'])")
    
    # Store token in environment variable for next steps
    echo "##vso[task.setvariable variable=DATABRICKS_TOKEN]$ACCESS_TOKEN"
    
    echo "OAuth token generated successfully"
  displayName: 'Generate OAuth Token'

- script: |
    # Configure Databricks CLI with the OAuth token
    echo "Configuring Databricks CLI..."
    mkdir -p ~/.databrickscfg
    
    # Create config file
    cat > ~/.databrickscfg << EOF
    [DEFAULT]
    host = $(DATABRICKS_HOST)
    token = $(DATABRICKS_TOKEN)
    EOF
    
    # Test connection
    echo "Testing Databricks connection..."
    databricks fs ls
  displayName: 'Configure and Test Databricks CLI'

- script: |
    # Create DBFS directory and copy files
    echo "Creating DBFS directory..."
    databricks fs mkdirs "dbfs:/FileStore/deploymentt"
    
    # Add your file copy commands here
    # Example:
    # databricks fs cp -r source_folder dbfs:/FileStore/deployment/
  displayName: 'Copy Files to DBFS'