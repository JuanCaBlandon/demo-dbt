{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pyspark.sql.functions import col, lit, coalesce, md5, concat_ws\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "env = dbutils.widgets.get(\"environment\")\n",
    "start_date = dbutils.widgets.get(\"start_date\")\n",
    "execution_date = dbutils.widgets.get(\"execution_date\")\n",
    "destination_path = f\"state_reporting_{env}.silver.rt_1_2\"\n",
    "\n",
    "\n",
    "def get_previous_events():\n",
    "    \"\"\"Get previous events for both record types\"\"\"\n",
    "    return spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            drivers_license_number,\n",
    "            record_type,\n",
    "            CAST(MAX(event_date) AS TIMESTAMP) as event_date\n",
    "        FROM state_reporting_{env}.silver.rt_1_2\n",
    "        WHERE record_type IN (1, 2)\n",
    "        GROUP BY drivers_license_number, record_type\n",
    "    \"\"\")\n",
    "\n",
    "def get_violation_events(record_type):\n",
    "    \"\"\"Get violation events based on record type\"\"\"\n",
    "    lookback_count = 4 if record_type == 1 else 9\n",
    "    time_window = \"24 HOURS\" if record_type == 1 else \"30 DAYS\"\n",
    "    min_violations = 5 if record_type == 1 else 10\n",
    "    \n",
    "    return spark.sql(f\"\"\"\n",
    "        WITH base_data AS (\n",
    "            SELECT\n",
    "                cec.event_dw_id,\n",
    "                cec.customer_id,\n",
    "                cc.drivers_license_number,\n",
    "                cec.device_usage_violation_id,\n",
    "                cec.device_usage_event_violation_id,\n",
    "                cec.customer_transaction_id,\n",
    "                cec.event_type,\n",
    "                CAST(cec.event_date AS TIMESTAMP) as event_date\n",
    "            FROM state_reporting_{env}.silver.customer_events_cleaned cec\n",
    "            INNER JOIN state_reporting_{env}.silver.customer_cleaned cc \n",
    "                ON cc.customer_id = cec.customer_id\n",
    "                AND cc.is_inconsistent = 0\n",
    "            INNER JOIN state_reporting_{env}.silver.batch_customer_cleaned AS bcc\n",
    "                ON cc.drivers_license_number = bcc.drivers_license_number\n",
    "                AND RIGHT(bcc.vin,6) = RIGHT(cc.vin,6)\n",
    "                AND bcc.created_at = '{execution_date}'\n",
    "                AND bcc.is_inconsistent = 0\n",
    "                AND bcc.repeat_offender = 1\n",
    "                AND bcc.offense_date >= '{start_date}'\n",
    "            WHERE cec.is_inconsistent = 0\n",
    "            AND cec.event_type = 'TYPE 1-2'\n",
    "        )\n",
    "        SELECT\n",
    "            event_dw_id,\n",
    "            customer_id,\n",
    "            drivers_license_number,\n",
    "            device_usage_violation_id,\n",
    "            CAST(LAG(event_date, {lookback_count}) OVER (\n",
    "                PARTITION BY drivers_license_number\n",
    "                ORDER BY event_date\n",
    "            ) AS TIMESTAMP) AS event_start_date,\n",
    "            CAST(event_date AS TIMESTAMP) as event_date\n",
    "        FROM base_data\n",
    "        QUALIFY COUNT(*) OVER (\n",
    "            PARTITION BY drivers_license_number\n",
    "            ORDER BY event_date\n",
    "            RANGE BETWEEN INTERVAL {time_window} PRECEDING AND CURRENT ROW\n",
    "        ) >= {min_violations}\n",
    "    \"\"\")\n",
    "\n",
    "def process_violations(events_df, previous_events_df, record_type):\n",
    "    \"\"\"Process violations for a specific record type\"\"\"\n",
    "    # Filter previous events for current record type\n",
    "    prev_events_filtered = previous_events_df[previous_events_df['record_type'] == record_type]\n",
    "    \n",
    "    # Create lookup dictionary for the current record type\n",
    "    last_events_dict = dict(zip(\n",
    "        prev_events_filtered['drivers_license_number'],\n",
    "        prev_events_filtered['event_date'].apply(pd.Timestamp)\n",
    "    ))\n",
    "    \n",
    "    result_schema = {\n",
    "        \"record_dw_id\": str,\n",
    "        \"event_dw_id\": str,\n",
    "        \"drivers_license_number\": str,\n",
    "        \"customer_id\": int,\n",
    "        \"event_id_type\": str,\n",
    "        \"event_id\": int,\n",
    "        \"event_date\": \"datetime64[ns]\",\n",
    "        \"record_type\": int,\n",
    "        \"record_description\": str\n",
    "    }\n",
    "    \n",
    "    marked_violations = pd.DataFrame(columns=result_schema.keys())\n",
    "    \n",
    "    for row in events_df.itertuples(index=False):\n",
    "        # Get last event date, default to 2025-01-01 if not found\n",
    "        last_event_date = last_events_dict.get(\n",
    "            row.drivers_license_number, \n",
    "            pd.Timestamp(\"2025-01-01\")\n",
    "        )\n",
    "        \n",
    "        current_event_start_date = pd.Timestamp(row.event_start_date)\n",
    "        current_event_date = pd.Timestamp(row.event_date)\n",
    "        \n",
    "        if current_event_start_date > last_event_date:\n",
    "            record_description = \"24 hrs\" if record_type == 1 else \"30 days\"\n",
    "            \n",
    "            new_row = {\n",
    "                \"record_dw_id\": str(row.record_dw_id),\n",
    "                \"event_dw_id\": str(row.event_dw_id),\n",
    "                \"drivers_license_number\": str(row.drivers_license_number),\n",
    "                \"customer_id\": int(row.customer_id),\n",
    "                \"event_id_type\": \"device_usage_violation_id\",\n",
    "                \"event_id\": int(row.device_usage_violation_id),\n",
    "                \"event_date\": current_event_date,\n",
    "                \"record_type\": record_type,\n",
    "                \"record_description\": record_description\n",
    "            }\n",
    "            \n",
    "            marked_violations = pd.concat([marked_violations, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            last_events_dict[row.drivers_license_number] = current_event_date\n",
    "    \n",
    "    return marked_violations\n",
    "\n",
    "def save_violations(violations_df):\n",
    "    print(\"saving\")\n",
    "    existing_table_schema = spark.table(f\"state_reporting_{env}.silver.rt_1_2\").schema\n",
    "    final_df = violations_df.select([col(field.name).cast(field.dataType) for field in existing_table_schema])\n",
    "    display(final_df)\n",
    "    try:\n",
    "        row_count = final_df.count()\n",
    "        final_df.write.format(\"delta\").mode(\"append\").saveAsTable(destination_path)\n",
    "        logger.info(f\"Successfully added {row_count} rows to {destination_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to Delta table: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Get previous events for both record types\n",
    "    previous_events = get_previous_events()\n",
    "    previous_events_df = previous_events.toPandas()\n",
    "    \n",
    "    # Process both types of violations\n",
    "    all_violations = []\n",
    "    \n",
    "    for record_type in [1, 2]:  # Process both record types\n",
    "        # Get events for current record type\n",
    "        base_df = get_violation_events(record_type)\n",
    "        base_df = base_df.withColumn(\n",
    "            \"record_dw_id\",\n",
    "            md5(concat_ws(\"_\", col(\"event_dw_id\"), lit(str(record_type)))).cast(\"string\")\n",
    "        )\n",
    "        \n",
    "        # Convert to pandas and process\n",
    "        events_df = base_df.toPandas()\n",
    "        \n",
    "        if not events_df.empty:\n",
    "            violations = process_violations(events_df, previous_events_df, record_type)\n",
    "            if not violations.empty:\n",
    "                all_violations.append(violations)\n",
    "    \n",
    "    # Combine all violations\n",
    "    if all_violations:\n",
    "        final_violations = pd.concat(all_violations, ignore_index=True)\n",
    "        final_violations_df = spark.createDataFrame(final_violations)\n",
    "        save_violations(final_violations_df)\n",
    "    else:\n",
    "        print(\"No violations found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
